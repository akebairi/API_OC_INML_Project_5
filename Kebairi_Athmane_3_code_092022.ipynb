{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb04448d-9cb2-4f56-b174-24592edab520",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Projet_OC_05 : Catégorisez automatiquement des questions (API)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8462dd8-d1e0-42be-82cb-7d550ad72db5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sommaire :\n",
    "\n",
    "**Partie 1 : Configuration du notebook**\n",
    "\n",
    " - <a href=\"#C11\">P1.1 : Chargement des librairies </a>\n",
    " - <a href=\"#C12\">P1.2 : Fonctions </a>\n",
    " - <a href=\"#C13\">P1.3 : Classes </a>\n",
    " - <a href=\"#C14\">P1.4 : Chargement des données</a>\n",
    " \n",
    "**Partie 2 : Modèle et déploiement**\n",
    "\n",
    " - <a href=\"#C21\">P2.1 : Modèle </a>\n",
    " - <a href=\"#C22\">P2.2 : Déploiement du modèle (mlflow) </a>\n",
    " - <a href=\"#C23\">P2.3 : Essai du modèle (mlflow) </a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236f59f-6ba4-4943-bdc9-2e710b5b1b42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h1>Partie 1 : Configuration du notebook</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfa272-9ea7-4a7d-9a47-b2d7614c78b9",
   "metadata": {},
   "source": [
    "# <a name=\"C11\"> P1.1 : Chargement des librairies </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f7e1fe1-b89b-41cc-b419-03904c8f9ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression, Perceptron\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, jaccard_score, recall_score, accuracy_score\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import joblib\n",
    "from mlflow.models.signature import infer_signature\n",
    "import mlflow.sklearn \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacdf4e-5f6b-46d7-809b-68e60115a160",
   "metadata": {},
   "source": [
    "**La liste des librairies ci-dessus sont chargées.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71462af7-6fb3-49c0-92f5-ce91887424da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a name=\"C12\"> P1.2 : Fonctions </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a030ba-fb61-4af8-a478-3641be4498d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(scores, y_test, y_pred, clf, average='weighted'):\n",
    "    \n",
    "    '''\n",
    "    print_score(scores, y_test, y_pred, clf, average='weighted')\n",
    "    \n",
    "    This function evaluates model performance using 4 scores: accuracy_score, recall_score, f1_score, jaccard_score\n",
    "    \n",
    "    parameters:\n",
    "        score: list\n",
    "            list appends all scores, useful to compare several models \n",
    "        y_test: list \n",
    "            true target\n",
    "        y_pred: lisy\n",
    "            predictions\n",
    "        average: str, default 'weighted'\n",
    "            average use dto compute final score\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    score_1 = accuracy_score(y_test, y_pred)\n",
    "    score_2 = recall_score(y_test, y_pred, average=average)\n",
    "    score_3 = f1_score(y_pred, y_test, average=average)\n",
    "    score_4 = jaccard_score(y_test, y_pred, average=average)\n",
    "\n",
    "\n",
    "    print(\"Clf: \", clf.__class__.__name__)\n",
    "    print(\"accuracy_score: {}\".format(score_1))\n",
    "    print(\"recall_score: {}\".format(score_2))\n",
    "    print(\"f1_score: {}\".format(score_3))\n",
    "    print(\"Jaccard score: {}\".format(score_4))\n",
    "\n",
    "    scores_temp = np.concatenate((scores, np.around([score_1, score_2, score_3, score_4], 2)))\n",
    "\n",
    "    return scores_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff44ca-6fe5-40f2-9976-cb7aeac391e0",
   "metadata": {},
   "source": [
    "**La fonction ci-dessus evalue les performance de modèle en utilisant 4 scores: accuracy, recall, f1_score, et jaccard .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962a81f4-2747-46cb-9137-11c020b43977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_process(sentence, most_freq_tags):\n",
    "    \n",
    "    '''\n",
    "    Function tags_process(sentence, most_freq_tags)\n",
    "    \n",
    "    This function removes < and > from a sentence and filter words, only keeping most_freq_tags\n",
    "    \n",
    "    parameters:\n",
    "        sentence: str\n",
    "            String to be cleaned \n",
    "        most_freq_tags: list \n",
    "            list of words (string) containing the words to be kept \n",
    "    '''\n",
    "    \n",
    "    sentence_process = (sentence.replace('<', ' ').replace('>', ' ').replace('/', ' ').strip()).split()\n",
    "    \n",
    "    sentence_filter = [word for word in sentence_process if word in most_freq_tags]                \n",
    "    \n",
    "    if sentence_filter:\n",
    "        return ' '.join(sentence_filter)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb92bb-eeb5-4058-a3d0-46889ed841e1",
   "metadata": {},
   "source": [
    "**La fonction ci-dessus assure le traitement des Tags. Elle supprimer \"<\" et \">\" et filtre les tags les plus fréquents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c381d916-13c5-4902-972e-ebec054f6fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_ponc_process(sentence):\n",
    "    \n",
    "    '''\n",
    "    tag_ponc_process(sentence)\n",
    "    \n",
    "    This function replaces the pnctuations in the 50-most used tags.  \n",
    "    \n",
    "    parameters:\n",
    "        sentence: str\n",
    "            String to be cleaned \n",
    "    '''\n",
    "    \n",
    "    return sentence.replace('c#', 'csharp').replace('c++', 'cplusplus').replace('.net', 'dotnet').replace('objective-c', 'objectivec').replace('ruby-on-rails', 'rubyonrails')\\\n",
    "                .replace('sql-server', 'sqlserver').replace('node.js', 'nodedotjs').replace('aspdotnet-mvc', 'aspdotnetmvc').replace('visual-studio', 'visualstudio').replace('visual studio', 'visualstudio')\\\n",
    "                .replace('unit-testing', 'unittesting').replace('cocoa-touch', 'cocoatouch').replace('python-3.x', 'python3x').replace('entity-framework', 'entityframework')\\\n",
    "                .replace('language-agnostic', 'languageagnostic').replace('amazon-web-services', 'amazonwebservices').replace('google-chrome', 'googlechrome').replace('user-interface', 'userinterface')\\\n",
    "                .replace('design-patterns', 'designpatterns').replace('version-control', 'versioncontrol').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b39854-16de-4dac-b81f-d71e9f62a9e6",
   "metadata": {},
   "source": [
    "**La fonction ci-dessus remplace ou supprime les ponctuations, dans les tags les plus fréquents, pour éviter de perdre ces tags lors de traitement de text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d21b4b1-5066-495e-a22d-283a7e068ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_tag_ponc_process(sentence):\n",
    "    \n",
    "    '''\n",
    "    inverse_tag_ponc_process(sentence)\n",
    "    \n",
    "    This function inverses the processing of the \"tag_ponc_process()\" function. It coverts the 50-most used tags into their original formats.  \n",
    "    \n",
    "    parameters:\n",
    "        sentence: str\n",
    "            String to be cleaned \n",
    "    '''\n",
    "    \n",
    "    return sentence.replace('csharp', 'c#').replace('cplusplus', 'c++').replace('dotnet', '.net').replace('objectivec', 'objective-c').replace('rubyonrails', 'ruby-on-rails')\\\n",
    "                .replace('sqlserver', 'sql-server').replace('nodedotjs', 'node.js').replace('aspdotnetmvc', 'aspdotnet-mvc').replace('visualstudio', 'visual-studio')\\\n",
    "                .replace('unittesting', 'unit-testing').replace('cocoatouch', 'cocoa-touch').replace('python3x', 'python-3.x').replace('entityframework', 'entity-framework')\\\n",
    "                .replace('languageagnostic', 'language-agnostic').replace('amazonwebservices', 'amazon-web-services').replace('googlechrome', 'google-chrome').replace('userinterface', 'user-interface')\\\n",
    "                .replace('designpatterns', 'design-patterns').replace('versioncontrol', 'version-control').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8811d9-4459-46fc-b675-307b9daff6d7",
   "metadata": {},
   "source": [
    "**La fonction ci-dessus inverse le traitement de la fonction \"tag_ponc_process()\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c3acd70-09c8-4fc7-accc-ef046b4e6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_process(sentence, stop_words, authorized_pos, no_pos_tag_list, no_lem_stem_list, force_is_alpha=False, method='spacy', lem_or_stem='lem'):\n",
    "    \n",
    "    '''\n",
    "    txt_process(sentence, stop_words, authorized_pos, no_pos_tag_list, no_lem_stem_list, force_is_alpha=False, method='spacy', lem_or_stem='lem')\n",
    "    \n",
    "    This function is a set of text processing steps: lower, html tags\\abbreviation\\ponctuation\\stop_words removing, tokonization, pos_tags filetring, and lemmatization\\stemming.  \n",
    "    \n",
    "    parameters:\n",
    "        sentence: str\n",
    "            String to be cleaned \n",
    "        stop_words: list\n",
    "            stop_words to be removed\n",
    "        authorized_pos: list\n",
    "            pos_tags to be kept\n",
    "        no_pos_tag_list: list\n",
    "            contains tokens, needed to be kept (ex: targets)\n",
    "        no_lem_stem_list: list\n",
    "            contains tokens, needed to be kept in their original formats (ex: Tags, keywords, targets)\n",
    "        force_is_alpha: bool, default False\n",
    "            if True, only alphabetic tokens are kept\n",
    "        method: str, default 'spacy'\n",
    "            defines the tokenization method: 'word'==>word_tokenize(), 'wordpunct'==>wordpunct_tokenize, 'spacy'==>nlp\\spacy\n",
    "        lem_or_stem: str, default 'lem'\n",
    "            choice between lemmatization or stemming: 'lem'==> lemmatization, 'stem'==> stemming\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    sentence_lower = sentence.lower()\n",
    "    \n",
    "    sentence_no_html_raw = BeautifulSoup(sentence_lower, \"html.parser\")\n",
    "\n",
    "    for data in sentence_no_html_raw(['style', 'script', 'code', 'a']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "        \n",
    "    sentence_no_html = ' '.join(sentence_no_html_raw.stripped_strings)\n",
    "    \n",
    "    sentence_no_abb = sentence_no_html.replace(\"what's\", \"what is \").replace(\"\\'ve\", \" have \").replace(\"can't\", \"can not \").replace(\"n't\", \" not \").replace(\"i'm\", \"i am \")\\\n",
    "                       .replace(\"\\'re\", \" are \").replace(\"\\'d\", \" would \").replace(\"\\'ll\", \" will \").replace(\"\\'scuse\", \" excuse \").replace(' vs ', ' ').replace('difference between', ' ')\n",
    "\n",
    "    sentence_no_abb_trans = tag_ponc_process(sentence_no_abb)\n",
    "\n",
    "    sentence_no_new_line = re.sub(r'\\n', ' ', sentence_no_abb_trans)\n",
    "\n",
    "    translator = str.maketrans(dict.fromkeys(string.punctuation, ' '))\n",
    "    sentence_no_caracter = sentence_no_new_line.translate(translator)\n",
    "    \n",
    "    sentence_no_stopwords = ' '.join([word for word in sentence_no_caracter.split() if word not in stop_words])\n",
    "    \n",
    "    if method=='word':\n",
    "        tokens_list = word_tokenize(sentence_no_stopwords)\n",
    "        sentence_tokens = [word for (word, tag) in nltk.pos_tag(tokens_list) if tag in authorized_pos and len(word)>=3 or word in no_pos_tag_list] \n",
    "    elif method=='wordpunct':\n",
    "        tokens_list = wordpunct_tokenize(sentence_no_stopwords)\n",
    "        sentence_tokens = [word for (word, tag) in nltk.pos_tag(tokens_list) if tag in authorized_pos and len(word)>=3 or word in no_pos_tag_list]\n",
    "    elif method=='spacy':\n",
    "        sentence_tokens =  [token.text for token in nlp(sentence_no_stopwords) if token.tag_ in authorized_pos and len(token.text)>=3 or token.text in no_pos_tag_list] \n",
    "    else: \n",
    "        tokens_list = RegexpTokenizer(r\"\\w+\").tokenize(sentence_no_stopwords)\n",
    "        sentence_tokens = [word for (word, tag) in nltk.pos_tag(tokens_list) if tag in authorized_pos and len(word)>=3 or word in no_pos_tag_list]\n",
    "    \n",
    "    if force_is_alpha:\n",
    "        alpha_tokens = [word for word in sentence_tokens if word.isalpha()]\n",
    "    else:\n",
    "        alpha_tokens = sentence_tokens\n",
    "    \n",
    "    if lem_or_stem=='lem':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lem_or_stem_tokens = [lemmatizer.lemmatize(word) if word not in no_lem_stem_list else word for word in alpha_tokens]\n",
    "        \n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "        lem_or_stem_tokens = [stemmer.stem(word) if word not in no_lem_stem_list else word for word in alpha_tokens]\n",
    "    \n",
    "    final_sentence = inverse_tag_ponc_process(' '.join(sentence_tokens))\n",
    "    \n",
    "    return final_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3597f4-fd38-4275-96c8-a5dca7872947",
   "metadata": {},
   "source": [
    "**La fonction ci-dessus assure le traitement de texte: suppression de html tags-ponctuation-stop_words-etc, tokennisation, lemmatisation-stemming, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51566532-a917-4678-87e8-7a5fdd9ed63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_USE_fct(sentences, b_size=8) :\n",
    "    \n",
    "    '''\n",
    "    feature_USE_fctsentences, b_size=8)\n",
    "    \n",
    "    This function extacts features from text, using USE (universal sentence encoder) model.  \n",
    "    \n",
    "    parameters:\n",
    "        sentence: str\n",
    "            String to be cleaned \n",
    "        b_size: int, default 8\n",
    "            sentence set treated at once, fixed to 8 in coherence with GPU architecture.\n",
    "    '''\n",
    "\n",
    "    batch_size = b_size\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        feat = embed(sentences[idx:idx+batch_size])\n",
    "\n",
    "        if step ==0 :\n",
    "            features = feat\n",
    "        else :\n",
    "            features = np.concatenate((features,feat))\n",
    "\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aadaa4f-d948-42e3-b2cc-798007a236cd",
   "metadata": {},
   "source": [
    "**La fonction ci-dessus utilise USE codage pour extraire des features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640cf9a2-2850-48c8-964a-681d4315a1a5",
   "metadata": {},
   "source": [
    "# <a name=\"C13\"> P1.3 : Classes </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c14ccb1d-babf-4c60-a2f0-145258a8b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TXTModel(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    '''\n",
    "    TXTModel(TransformerMixin, BaseEstimator)\n",
    "    \n",
    "    This classe groups a classifier \"clf\" and \"MultiLabelBinarizer\" transformer together. \"MultiLabelBinarizer\" is used in a post-processing step.  \n",
    "    \n",
    "    parameters:\n",
    "        clf: sklearn classifier\n",
    "            classifier to be used, ex: OneVsRestClassifier(LinearSVC()). \n",
    "        ml_binarizer: sklearn transformer\n",
    "            MultiLabelBinarizer() from sklearn.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, clf, ml_binarizer):\n",
    "        self.clf = clf\n",
    "        self.ml_binarizer = ml_binarizer\n",
    "        \n",
    "    def transform(self, Y):\n",
    "        \n",
    "        return self.ml_binarizer.transform(Y.tolist()) \n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        self.ml_binarizer.fit(Y.tolist())\n",
    "        self.clf.fit(X, self.ml_binarizer.transform(Y.tolist()))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        dfun = self.clf.decision_function(X)\n",
    "        most_common_idx = dfun.argsort()[:, -5:]\n",
    "        return self.classes_(most_common_idx)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "\n",
    "        return self.clf.decision_function(X)    \n",
    "    \n",
    "    def classes_(self, Y_idx):\n",
    "        \n",
    "        return self.ml_binarizer.classes_[Y_idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eeb1a7-0ded-48ab-abc7-6a9ae3c0017e",
   "metadata": {},
   "source": [
    "**La classe ci-dessus regroupe un classificateur et le transformateur MultiLabelBinarizer() pour assurer une multi-classification et un post-traitement du résultat.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238cb73-68e7-4719-86f7-abfaac666530",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a name=\"C14\"> P1.4 : Chargement des données </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58ff71e9-3bcd-4cd3-9d03-a1a9e3fd4b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Data size: (99997, 3)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Find Mime type of file or url using php for al...</td>\n",
       "      <td>&lt;p&gt;Hi I am looking for best way to find out mi...</td>\n",
       "      <td>&lt;php&gt;&lt;amazon-web-services&gt;&lt;mime-types&gt;&lt;content...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>native zlib inflate/deflate for swift3 on iOS</td>\n",
       "      <td>&lt;p&gt;I'd like to be able to inflate/deflate Swif...</td>\n",
       "      <td>&lt;ios&gt;&lt;swift&gt;&lt;swift3&gt;&lt;zlib&gt;&lt;swift-data&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>`Sudo pip install matplotlib` fails to find fr...</td>\n",
       "      <td>&lt;p&gt;I already have &lt;code&gt;matplotlib-1.2.1&lt;/code...</td>\n",
       "      <td>&lt;python&gt;&lt;numpy&gt;&lt;matplotlib&gt;&lt;homebrew&gt;&lt;osx-mave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Serialization in C# without using file system</td>\n",
       "      <td>&lt;p&gt;I have a simple 2D array of strings and I w...</td>\n",
       "      <td>&lt;c#&gt;&lt;sharepoint&gt;&lt;serialization&gt;&lt;moss&gt;&lt;wss&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I prevent IIS from compiling website?</td>\n",
       "      <td>&lt;p&gt;I have an ASP .NET web application which on...</td>\n",
       "      <td>&lt;asp.net&gt;&lt;performance&gt;&lt;web-services&gt;&lt;iis&gt;&lt;asmx&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Find Mime type of file or url using php for al...   \n",
       "1      native zlib inflate/deflate for swift3 on iOS   \n",
       "2  `Sudo pip install matplotlib` fails to find fr...   \n",
       "3      Serialization in C# without using file system   \n",
       "4       How do I prevent IIS from compiling website?   \n",
       "\n",
       "                                                Body  \\\n",
       "0  <p>Hi I am looking for best way to find out mi...   \n",
       "1  <p>I'd like to be able to inflate/deflate Swif...   \n",
       "2  <p>I already have <code>matplotlib-1.2.1</code...   \n",
       "3  <p>I have a simple 2D array of strings and I w...   \n",
       "4  <p>I have an ASP .NET web application which on...   \n",
       "\n",
       "                                                Tags  \n",
       "0  <php><amazon-web-services><mime-types><content...  \n",
       "1             <ios><swift><swift3><zlib><swift-data>  \n",
       "2  <python><numpy><matplotlib><homebrew><osx-mave...  \n",
       "3         <c#><sharepoint><serialization><moss><wss>  \n",
       "4    <asp.net><performance><web-services><iis><asmx>  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt_data = pd.read_csv('data.csv')\n",
    "raw_txt_data = raw_txt_data.select_dtypes(include=object)\n",
    "raw_txt_data.dropna(inplace=True)\n",
    "\n",
    "print('-'*150)\n",
    "print('Data size:', raw_txt_data.shape)\n",
    "print('-'*150)\n",
    "raw_txt_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc8d06-2a30-45c7-a806-f27360aa4f3f",
   "metadata": {},
   "source": [
    "**Ci-dessus, les données originales sont chargées.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a682db-c27a-4bbe-8d05-c7e4426591cf",
   "metadata": {},
   "source": [
    "<h1>Partie 2 : Modèle et déploiement</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6de190-8141-4656-9ea8-22dc3c4a6a21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a name=\"C21\"> P2.1 : Modèle </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fce1b45-3b8e-4fff-8d74-02c10b557ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_val = 50\n",
    "all_tags = ' '.join(raw_txt_data.Tags.apply(lambda sentence: sentence.replace('<', ' ').replace('>', ' ')).tolist()).split()\n",
    "unique_tags = list(set(all_tags))\n",
    "keywords = nltk.FreqDist(all_tags)\n",
    "most_common_tags = [word[0] for word in keywords.most_common(most_common_val)]\n",
    "\n",
    "raw_txt_data['Tags'] = raw_txt_data.Tags.apply(lambda sentence: tags_process(sentence, most_common_tags))\n",
    "raw_txt_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfab81a-2909-42cb-bdf4-da8bbe4ddd98",
   "metadata": {},
   "source": [
    "**La variable \"Tags\" est nétoyée et filtrée: 50 tags les plus fréquents sont uniquement gardés.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86e92540-6ba9-4009-a636-baade5686256",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "authorized_pos = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "no_pos_tag_list = tag_ponc_process(' '.join(most_common_tags)).split()\n",
    "no_lem_stem_list = tag_ponc_process(' '.join(most_common_tags)).split()\n",
    "stop_words = list(set(stopwords.words('english'))) + \\\n",
    "                ['[', ']', ',', '.', ':', '?', '(', ')']\n",
    "stop_words.extend(['good', 'idea', 'solution', 'issue', 'problem', 'way', 'example', 'case', 'question', 'questions', 'something', 'everything',\n",
    "                   'anything', 'thing', 'things', 'answer', 'thank', 'thanks', 'none', 'end', 'anyone', 'test', 'lot', 'one', 'someone', 'help'])\n",
    "\n",
    "clf = OneVsRestClassifier(LinearSVC())\n",
    "ml_binarizer = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a4d47-adb1-4c8a-ad9c-1a3003553540",
   "metadata": {},
   "source": [
    "**La définition des variables du modèle est faite ci-dessus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cb0190e-149f-42b7-9a1b-d8540edefc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_txt_data['Title_Body'] = raw_txt_data['Title'] + ' ' + raw_txt_data['Body']\n",
    "\n",
    "X = raw_txt_data['Title_Body'].sample(frac=0.01).apply(lambda sen: txt_process(sen, stop_words, authorized_pos, no_pos_tag_list, no_lem_stem_list))\n",
    "y = pd.Series(raw_txt_data.Tags.values, index=raw_txt_data.Tags.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a51645-9915-4d31-be2a-3ed0e2bece4b",
   "metadata": {},
   "source": [
    "**Test processing et création des variables X, y.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0f00b76-66e2-485d-8545-785fc0ff73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_norm = 8\n",
    "X_use = feature_USE_fct(X, idx_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e761f3-113b-4009-977c-2f4fc8272024",
   "metadata": {},
   "source": [
    "**Création des features USE est faite ci-dessus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "e03d6eeb-6c04-4960-9756-7e0f13ac33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_use, y[:(len(X_use)//idx_norm)*idx_norm], test_size = 0.25, random_state = 1) \n",
    "\n",
    "X_train = X_train[:(len(X_train)//idx_norm)*idx_norm]\n",
    "X_test = X_test[:(len(X_test)//idx_norm)*idx_norm]\n",
    "y_train = y_train[:(len(y_train)//idx_norm)*idx_norm]\n",
    "y_test = y_test[:(len(y_test)//idx_norm)*idx_norm]\n",
    "\n",
    "y_train = y_train.apply(lambda x: x.split())\n",
    "y_test = y_test.apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101bc366-2855-4ec1-b4fc-284ed06255bd",
   "metadata": {},
   "source": [
    "**Split des variable X, y.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1b36c426-7a28-4cc4-9f7a-d2dd7739c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TXTModel(clf, ml_binarizer)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4a611-d443-4ae0-90f5-ecb627d04453",
   "metadata": {},
   "source": [
    "**Ci-dessus, fitting du modèle.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ead9b81e-607b-4075-8908-e2eedc884ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:  TXTModel\n",
      "accuracy_score: 0.0004152823920265781\n",
      "recall_score: 0.2968069398301956\n",
      "f1_score: 0.4475522529292254\n",
      "Jacard score: 0.2882881267815206\n"
     ]
    }
   ],
   "source": [
    "ml_binarizer_score = MultiLabelBinarizer()\n",
    "ml_binarizer_score.fit(y_train)\n",
    "scores = print_score([], ml_binarizer_score.transform(clf.predict(X_test)), ml_binarizer_score.transform(y_test), clf, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7c40a-3bed-4b3b-ac32-12399ec14ca5",
   "metadata": {},
   "source": [
    "**Le score du modèle est donné ci-dessus. Le score est inférieur, car obtenu avec la méthode \"decision_function\" en considérant les 5 premiers Tags.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c4b97-8c27-4409-a3da-6670bef8bd94",
   "metadata": {},
   "source": [
    "# <a name=\"C22\"> P2.2 : déploiement du modèle (mlflow)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "664108c6-4967-4b4d-81d7-04e13b4ec476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf_housing.joblib']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'clf_housing.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73d466-82dc-4f87-8488-86a0ca3c3dfb",
   "metadata": {},
   "source": [
    "**Sérialisation du modèle à l'aide de joblib.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6b865cb2-a7ae-48ae-b448-e9a1338bfb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9100e13-4816-4bf3-b1d0-80545370b8ea",
   "metadata": {},
   "source": [
    "**Extraction de la signature de données d'entrée et de sortie.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "87c578e7-080c-4a7e-a6d1-6cbf60b667e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.save_model(clf, 'mlflow_model', signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359b300-e29d-49f4-b519-7e721cb7a0c7",
   "metadata": {},
   "source": [
    "**Le modèle \"clf\" est sauvegardé à l'aide de la fonction save_model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b41896-b661-4ce9-a7a4-fbdf865add3d",
   "metadata": {},
   "source": [
    "mlflow models serve -m mlflow_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532da0af-df48-4186-a930-e2443657e2b7",
   "metadata": {},
   "source": [
    "**Une API REST (mlflow) est créée.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1ad41-e226-4f5e-aeb3-5cd336231702",
   "metadata": {},
   "source": [
    "# <a name=\"C23\"> P2.3 : Essai du modèle (mlflow)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de175f6e-3f8f-4845-8764-e9fb0ae57a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'storing raw json in redis by using spring-data-redis'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idx = np.random.randint(0, raw_txt_data.shape[0], size=1)[0]\n",
    "raw_txt_data['Title'].iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cad7b84d-4bbb-4247-8bad-041b315af99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>I am using RedisCacheManager to store my cache data in my spring-boot application. Default serializer seems to serialize everything into byte and deserialize from byte to appropriate java type. </p>\\n\\n<p>However, I want to make the cache data be stored as json so that I can read it from none-java clients.</p>\\n\\n<p>I found that switching from default one to other serializers such as Jackson2JsonRedisSerializer supposed to work. After doing this, deserialization phase fails.</p>\\n\\n<p>pom.xml</p>\\n\\n<pre><code>    &lt;dependency&gt;\\n        &lt;groupId&gt;org.springframework.data&lt;/groupId&gt;\\n        &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt;\\n    &lt;/dependency&gt;\\n\\n    &lt;dependency&gt;\\n        &lt;groupId&gt;redis.clients&lt;/groupId&gt;\\n        &lt;artifactId&gt;jedis&lt;/artifactId&gt;\\n    &lt;/dependency&gt;\\n</code></pre>\\n\\n<p>CacheConfig.java</p>\\n\\n<pre><code>@Configuration\\n@EnableCaching\\npublic class CacheConfig {\\n\\n    @Bean\\n    public RedisConnectionFactory createRedisConnectionFactory() {\\n        JedisConnectionFactory factory = new JedisConnectionFactory();\\n        factory.setHostName(\"localhost\");\\n        return factory;\\n    }\\n\\n//    SPRING-DATA-REDIS ALREADY PROVIDES A STRING REDIS TEMPLATE, SO THE FOLLOWING IS NOT NECESSARY\\n//    @Bean\\n//    public RedisTemplate&lt;String, String&gt; createRedisTemplate(RedisConnectionFactory factory) {\\n//        RedisTemplate&lt;String, String&gt; redisTemplate = new RedisTemplate&lt;&gt;();\\n//        redisTemplate.setConnectionFactory(factory);\\n//        return redisTemplate;\\n//    }\\n\\n    @Bean\\n    public CacheManager redisCacheManager(RedisTemplate redisTemplate) {\\n        RedisCacheManager cacheManager = new RedisCacheManager(redisTemplate);\\n        return cacheManager;\\n    }\\n}\\n</code></pre>\\n\\n<p>Is there a way to store them in a pure JSON format and successfully deserialize from it?</p>\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt_data['Body'].iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eea463-a028-453a-9694-b13937ba3817",
   "metadata": {},
   "source": [
    "**Ci-dessus, un titre et une question sont générés pour tester l'API.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "919e882d-fe1c-4284-8695-cba88896dcc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'java json spring'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt_data['Tags'].iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ea68e-f6f4-4448-b16f-279c7f7cb409",
   "metadata": {},
   "source": [
    "**La vrai target est donnée ci-dessus.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
